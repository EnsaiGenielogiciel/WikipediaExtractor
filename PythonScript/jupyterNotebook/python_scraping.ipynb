{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD LIBRARIES\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASS TO EXTRACT TABLE IN WIKIPEDIA WEBPAGE\n",
    "class WikipediaHTMLExtractor:\n",
    "\n",
    "    def __init__(self, link_):\n",
    "        super(WikipediaHTMLExtractor).__init__()\n",
    "        self.link = \"https://en.wikipedia.org/wiki/\"+ link_\n",
    "        self.list_data_frame = list()\n",
    "        \n",
    "\n",
    "    # Request a webpage and send back a soup\n",
    "    def scraping(self):\n",
    "\n",
    "        requete = requests.get(self.link)\n",
    "\n",
    "        if requete.status_code ==200:\n",
    "            html_parser = BeautifulSoup(requete.text, 'html.parser')\n",
    "            \n",
    "            tables = html_parser.find_all(\"table\", {\"class\":  \"wikitable\"})         \n",
    "            \n",
    "            return tables if len(tables) !=0 else None\n",
    "            \n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    \n",
    "    # Delete all <sup></sup> in table \n",
    "    def decompose_sup(self,parent):\n",
    "        sup_tag = parent.find_all('sup')\n",
    "        if len(sup_tag)>0:\n",
    "            for i in range(len(sup_tag)):\n",
    "                parent.sup.decompose()\n",
    "            return parent\n",
    "        else:\n",
    "            return parent\n",
    "\n",
    "\n",
    "    # Create csv using pandas dataframe\n",
    "    def data_to_csv(self, root_path):\n",
    "\n",
    "        if self.list_data_frame:\n",
    "            \n",
    "            name_file = self.link.split('/')[-1]\n",
    "\n",
    "            for i, data in enumerate(self.list_data_frame):\n",
    "                \n",
    "                data.to_csv(root_path+'/'+name_file+'_file_'+str(i)+'.csv', encoding='utf-8', index=False)\n",
    "        \n",
    "        else:\n",
    "            print(\"Any data to save\")\n",
    "\n",
    "    \n",
    "    # Create row using th and td in a tr\n",
    "    def create_row(self, soup_tr):\n",
    "        \n",
    "        rows = list()\n",
    "        for row_num, table_row in enumerate(soup_tr):\n",
    "            rows.append(list())\n",
    "\n",
    "            for cell in table_row.find_all(['th','td']):\n",
    "        \n",
    "                cell = self.decompose_sup(cell)\n",
    "\n",
    "                rows[row_num].append(cell.text.rstrip('\\n'))\n",
    "       \n",
    "        return rows\n",
    "\n",
    "    # Uniformisation of number of cells per rows\n",
    "    def uniformisation(self, rows):\n",
    "        \n",
    "        #for some tables, we have rows containing cells whose colspan attribute value is \n",
    "        #such that cell_num + colspan is greater than the number of columns. We have also\n",
    "        #rows which contains less <td></td> than the number of columns. So at the end\n",
    "        # we truncate the length of rows or fill it according to the number of columns (length or rows[0])\n",
    "        nb_col = len(rows[0])   \n",
    "        for i, elt in enumerate(rows):\n",
    "            if len(elt)!=nb_col:\n",
    "                if len(elt)<nb_col:  \n",
    "                    rows[i] = rows[i] + ['']*(nb_col-len(elt))\n",
    "                else:\n",
    "                    rows[i] = rows[i][:nb_col]\n",
    "        return rows      \n",
    "\n",
    "    # Manage colspan and update rows\n",
    "    def fill_cell_span(self, rows, soup_tr, fill_span):\n",
    "\n",
    "        for row_num, table_row in enumerate(soup_tr):\n",
    "            \n",
    "            index_row_cell= -1\n",
    "            index_col_cell =-1\n",
    "            for cell_num, cell in enumerate(table_row.find_all(['th','td'])):\n",
    "                \n",
    "                index_row_cell += 1 \n",
    "                cell = self.decompose_sup(cell)\n",
    "                if 'colspan' in cell.attrs and cell.attrs['colspan'].isdigit():\n",
    "                    colspan = int(cell.attrs['colspan'])\n",
    "                    for i in range(1, colspan):\n",
    "                        index_row_cell += 1\n",
    "                        rows[row_num].insert(\n",
    "                            index_row_cell, cell.text.rstrip(\"\\n\") if fill_span else \"\"\n",
    "                        )\n",
    "\n",
    "                index_col_cell += 1\n",
    "                if 'rowspan' in cell.attrs and cell.attrs['rowspan'].isdigit():\n",
    "                    rowspan = int(cell.attrs['rowspan'])\n",
    "                    \n",
    "                    for i in range(1, rowspan):\n",
    "                        index_col_cell +=1\n",
    "                        ## rowspan < len(soup_tr)  if row_num+i <=len(soup_tr)\n",
    "                        if row_num+i < len(soup_tr):\n",
    "                            rows[row_num+i].insert(cell_num , cell.text.rstrip('\\n') if fill_span else '')\n",
    "\n",
    "        rows = self.uniformisation(rows)\n",
    "        return rows\n",
    "\n",
    "\n",
    "    # Put all together to build and save data\n",
    "    def build_data(self, fill_span=True):\n",
    "\n",
    "        tables = self.scraping()\n",
    "\n",
    "        if tables is not None:\n",
    "            \n",
    "            count_table = 0 # count the number of table in the dataframe\n",
    "\n",
    "            for tab in tables:\n",
    "                \n",
    "                soup_tr = tab.find_all('tr')\n",
    "\n",
    "                if len(soup_tr) != 0:\n",
    "                    \n",
    "                    count_table += 1    # increment only if the soup_tr isn't empty\n",
    "\n",
    "                    # create row\n",
    "                    rows =  self.create_row(soup_tr)\n",
    "\n",
    "                    # manage span of the row\n",
    "                    rows = self.fill_cell_span(rows, soup_tr, fill_span)\n",
    "\n",
    "                    # get column name\n",
    "                    col = rows.pop(0)\n",
    "            \n",
    "                    # create dataframe and update list_data_frame\n",
    "                    data = pd.DataFrame(rows, columns=col) \n",
    "\n",
    "                    self.list_data_frame.append(data)\n",
    "\n",
    "            print('INFO : Scraping completed successfully')\n",
    "            return count_table\n",
    "        else:\n",
    "            print(\"WARNING : The web page not exist probably\")\n",
    "            return 0\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Scraping completed successfully\n"
     ]
    }
   ],
   "source": [
    "## THE CLASS ON ONE LINK\n",
    "Obj = WikipediaHTMLExtractor(link_=\"Comparison_of_programming_languages_(syntax)\") \n",
    "Obj.build_data(fill_span=True)\n",
    "Obj.data_to_csv(root_path=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Statement separator-terminator</th>\n",
       "      <th>Secondary separator-terminator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABAP</td>\n",
       "      <td>period separated</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ada</td>\n",
       "      <td>semicolon terminated</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALGOL</td>\n",
       "      <td>semicolon separated</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ALGOL 68</td>\n",
       "      <td>semicolon and comma separated</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>APL</td>\n",
       "      <td>newline terminated</td>\n",
       "      <td>[Direct_function ⋄] separated Secondary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Language Statement separator-terminator  \\\n",
       "0      ABAP               period separated   \n",
       "1       Ada           semicolon terminated   \n",
       "2     ALGOL            semicolon separated   \n",
       "3  ALGOL 68  semicolon and comma separated   \n",
       "4       APL             newline terminated   \n",
       "\n",
       "            Secondary separator-terminator  \n",
       "0                                           \n",
       "1                                           \n",
       "2                                           \n",
       "3                                           \n",
       "4  [Direct_function ⋄] separated Secondary  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Obj.list_data_frame[0].head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9745286f125fc4c0e2f0c9b60f5bc2fc84157ad21e4a802e3ca82238288d0981"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
